---
title: "Modelling Latent Topics in Text Data"
description: "Module 3, Introduction to Computational Social Science (Python), GESIS Fall Seminar 2024"
author:
  - name: John McLevey
    url: https://johnmclevey.com
    email: john.mclevey@uwaterloo.ca
    corresponding: true
    affiliations:
      - name: University of Waterloo
date: "08/26/2024"
date-modified: last-modified
categories:
  - Python
  - GESIS
  - computational social science
  - data science
  - tutorial
tags:
  - Python
  - GESIS
  - computational social science
  - data science
  - tutorial
bibliography: references.bib
reference-location: margin
citation-location: margin
freeze: true
license: "CC BY-SA"
---

# Introduction

This tutorial is focused on modelling latent topics -- or hidden thematic structure -- in text data. We'll start our introduction to topic models by focusing on simple count-based methods for manifest content. Since these count-based approaches have a *very* long history pre-dating topics, I've characterized them here as part of the "pre-history" topic modelling. Understanding these approaches first is usually a good way to understand more complex models.

Once we've discussed count-based analyses of manifest content, we'll discuss a deterministic approach to topic modelling known as Latent Semantic Analysis (LSA), which was arguably the "first wave" of modern topic models.^[This historical framing accurately represents the development of topic models, but the history of applications is a little messier. The trajectory is more or less the same, but earlier models are still useful and continue to be used in many fields. The heuristic framing of first, second, and third wave models is my own.] Then we'll discuss probabilistic Latent Dirichlet Allocation (LDA) models, which represent the "second wave." Finally, we'll learn about transformer-based topic modells, specifically BERTopic, which represent the "third wave."

We'll use a corpus of political speeches from the British Parliament for most of this tutorial, and will shift to the YouTube data at the end.

## Learning Objectives

By the end of this tutorial, you should be able to:

1. Understand the Evolution of Topic Modeling
    - Describe the progression from simple count-based methods to advanced transformer-based models.
    - Explain how each wave of topic modeling improves upon the previous techniques.
2. Preprocess Text Data
    - Implement text preprocessing steps including tokenization, stopword removal, lemmatization, and bigram detection.
    - Construct document-term matrices and understand their role in topic modeling.
3. Develop Topic Models
    - Perform basic count-based analysis and difference of proportions analysis to explore manifest thematic differences across document groups.
    - Conduct a Latent Semantic Analysis (LSA) using Truncated Singular Value Decomposition (SVD) to discover latent topics in text data.
    - Develop probabilistic topic models, specifically Latent Dirichlet Allocation (LDA) models, and interpret the results.
    - Fit and interpret transformer-based topic models using BERTopic.
4. Visualize and Interpret Topic Models
    - Create meaningful visualizations to compare and interpret topics across different documents.
    - Evaluate the strengths and weaknesses of different topic modeling approaches.

# Setup

```{python}
import numpy as np
import pandas as pd
import spacy
import requests
import icsspy
from icsspy.text import preprocess, sparse_groupby, get_topic_word_scores, plot_topic_distribution
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
from bertopic import BERTopic
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import Normalizer
import scipy

from gensim import corpora
from gensim.models import LdaModel
from gensim.models.ldamulticore import LdaMulticore
from gensim.models.coherencemodel import CoherenceModel
import random

icsspy.set_style()
nlp = spacy.load("en_core_web_sm")
```

# The British Hansard

For the first part of this tutorial, we'll work with a collection of political speeches from the British Parliament, focusing on speeches given between 2016 and 2020. We'll load the data from the course package.

```{python}
bh1620 = icsspy.load_hansard(years=["2016-2020"], british=True)
bh1620.info()
```

The data subset we are working with contains `{python} len(bh1620)` speeches.

## Data Processing

Let's confirm that we have the correct years.

```{python}
bh1620['year'].min(), bh1620['year'].max()
```

Let's do some initial processing by selecting the subset of columns we are interested in and dropping rows that are missing data from crucial columns.

```{python}
bh1620 = bh1620[['speech', 'speakername', 'party', 'constituency', 'year']]
bh1620.dropna(subset=['party', 'speakername', 'speech'], inplace=True)
bh1620.info()
```

Dropping missing data leaves us with `{python} len(bh1620)` speeches.

## Party Breakdown

Let's take a look at the breakdown of speeches by party.

```{python}
bh1620['party'].value_counts().reset_index()
```

We'll focus only on speeches made by MPs from parties that collectively gave more than 400 speeches within our four-year window.

```{python}
parties_keep = [
    'Conservative',
    'Labour',
    'Scottish National Party',
    'Labour (Co-op)',
    'Liberal Democrat',
    'Democratic Unionist Party',
    'Plaid Cymru',
    'Green Party'
]

party_subset = bh1620[bh1620['party'].isin(parties_keep)].copy()
party_subset.reset_index(drop=True, inplace=True)

total_speech_counts = party_subset['party'].value_counts()
total_speech_counts
```

### Stratified Random Sample of Political Speeches

Given the size of the dataset, you may want to work with a smaller sample. We can draw a **stratified random sample** where the strata are political parties so that that each party is proportionally represented in our sample.

```{python}
sample_size_fraction = 0.1  # let's just take 10% for now

sampled_speeches = party_subset.groupby('party').sample(
    replace=False,
    frac=sample_size_fraction,
    random_state=23
)

len(sampled_speeches)
```

Our stratified random sample contains `{python} len(sampled_speeches)` speeches. Let's examine the party breakdown in our sample.

```{python}
sampled_speech_counts = sampled_speeches['party'].value_counts()

sample_sizes = pd.DataFrame(
    zip(total_speech_counts, sampled_speech_counts),
    columns=['Total', 'Sample'],
    index=parties_keep
)

sample_sizes
```

### Speech Length by Party

Let's compare the length of speeches across political parties by computing the number of tokens in each speech.

```{python}
sampled_speeches['speech_len'] = sampled_speeches['speech'].apply(
  lambda x: len(x.split(" "))
)

parties = sampled_speeches.groupby('party')
parties['speech_len'].median()
```

We can visualize party differences by plotting the kernel density estimate for speech length within each party.

```{python}
def party_subplot(subgroup, title, position):
    sns.kdeplot(ax=position, data=subgroup, x='speech_len',
                log_scale=True, fill=False, alpha=1, linewidth=3, color='C0')
    position.set(xlabel='Number of tokens (log scale)', title=title)

fig, ax = plt.subplots(2, 4, sharex=True, sharey=True, figsize=(20, 6))
party_subplot(parties.get_group('Conservative'), 'Conservative', ax[0, 0])
party_subplot(parties.get_group('Labour'), 'Labour', ax[0, 1])
party_subplot(parties.get_group('Scottish National Party'), 'Scottish National Party', ax[0, 2])
party_subplot(parties.get_group('Labour (Co-op)'), 'Labour (Co-op)', ax[0, 3])
party_subplot(parties.get_group('Liberal Democrat'), 'Liberal Democrat', ax[1, 0])
party_subplot(parties.get_group('Democratic Unionist Party'), 'Democratic Unionist Party', ax[1, 1])
party_subplot(parties.get_group('Plaid Cymru'), 'Plaid Cymru', ax[1, 2])
party_subplot(parties.get_group('Green Party'), 'Green Party', ax[1, 3])

plt.tight_layout()
plt.savefig('output/speech_length_by_party.png', dpi=300)
```

## Text Processing

Text processing is a crucial step before applying the kinds of text analysis techniques we will apply here.^[This is not the case for the transformer-based approaches we will consider later.] Common steps include removing stopwords, converting to lowercase, lemmatization, and detecting bigrams. We will use the preprocess() function from the course package to perform these operations.

Note that the code block below will take some time to run, primarily because of the bigram detection. You'll see several progress bars appear (one at a time) to give you a sense of how long this code needs to run on your machine.

```{python}
docs = sampled_speeches['speech']

bigram_model, processed_texts = preprocess(
    docs,
    nlp=nlp,
    bigrams=True,
    detokenize=True,
    n_process=1
)
```

We've now detected bigrams, filtered out stopwords, selected relevant tokens (nouns, proper nouns, and adjectives) using part-of-speech tagging, and lemmatized them. Let's add the preprocessed speech data to our dataframe.

::: { .callout-note }
Heads up, the code below will take a while to run! You'll see three progress bars appear (one at a time) to give you a sense of how it will take to run.
:::

```{python}
sampled_speeches['processed_text'] = processed_texts
```

We now have two Series with text data: the original full speech text and the processed version. I know the processed text doesn't look better from a human perspective, but it will help out these initial models quite a bit.

## Creating a Document-Term Matrix

Creating a document-term matrix (DTM) is the first step in many text analysis workflows. It allows us to represent our text data in a structured format where each document is represented as a vector of word counts or TF-IDF scores.

```{python}
count_vectorizer = CountVectorizer(
    max_df=0.1,
    min_df=3,
    strip_accents='ascii',
)

count_matrix = count_vectorizer.fit_transform(sampled_speeches['processed_text'])
vocabulary = count_vectorizer.get_feature_names_out()

count_matrix.shape
```

Now that we've processed our data, let's start digging into topic modeling, starting with the early deterministic methods and progressing to the latest transformer-based models!

# Topic Modeling Pre-history

## Counting and Comparing Manifest Content

We can start by exploring simple count-based methods and difference of proportions analysis. This provides a foundation for understanding more complex topic models.

```{python}
count_data = pd.DataFrame.sparse.from_spmatrix(count_matrix)
count_data.columns = vocabulary

count_data.index = sampled_speeches['party']
count_data.shape
```

Let's look at a random sample of our data (yes, another).

```{python}
party_counts = sparse_groupby(sampled_speeches['party'], count_matrix, vocabulary)
results = party_counts.div(party_counts.sum(axis=1), axis=0)
results_t = results.T
results_t.sample(20, random_state=10061986)
```

With this dataframe, we can compare the proportions of specific tokens across each party.

```{python}
search_term = 'scotland'
results_t.loc[search_term].sort_values(ascending=False)
```

We can also compute the difference of proportions between any pair of document groups, revealing which tokens are more associated with one group over another.

```{python}
diff_con_snp = results_t['Conservative'] - results_t['Scottish National Party']
diff_con_snp.sort_values(ascending=False, inplace=True)

con_not_snp = diff_con_snp.head(20)  # Conservatives but not SNP
lab_not_snp = diff_con_snp.tail(20)  # SNP but not Conservatives

dop = pd.concat([con_not_snp, lab_not_snp])
```

Let's visualize these differences.

```{python}
fig, ax = plt.subplots(figsize=(12, 12))
sns.swarmplot(x=dop, y=dop.index, color='black', size=8)
ax.axvline(0)  # add a vertical line at 0
plt.grid()  # add a grid to the plot to make it easier to interpret
ax.set(xlabel=r'($\longleftarrow$ Scottish National Party)        (Conservative Party $\longrightarrow$)', ylabel='', title='Difference of Proportions')
plt.tight_layout()
plt.show()
```

Plots like this one highlight differences in how groups of people talk, or groups of documents are written, based on relative differences in the words they use. In this specific case, our plot compares the difference in proportions of word usage between the SNP and the Conservatives.

The words we are comparing are on the y-axis. The x-axis shows the difference in proportions between the two parties, with a vertical line indicating 0. Negative values indicate words that are used more frequently by the SNP, whereas positive values indicate words more frequently used by the Conservatives.

Each dot represents a word, and its position on the x-axis indicates the extent to which that word is used by one party relative to the other. For example, words like "scotland", "scottish", and "brexit" are on the far left, indicating that these terms are more commonly used by the SNP than by the Conservative Party.^[Obviously, the positive and negative signs are arbitrary.] On the other hand, words like "eu", "policy", and "impact" are on the right side, suggesting these are more commonly used by the Conservative Party. The further a dot is from the red line (0), the larger the difference in usage between the two parties. Words closer to the red line (0) have less different usage and could be used similarly by both parties.

Try running this analysis this with other search terms!

# First Wave Topic Models: Latent Semantic Analysis (LSA)

Latent Semantic Analysis (LSA) is an early method for uncovering latent structures in text data. It uses dimensionality reduction methods -- specifically truncated Singular Value Decomposition (SVD) -- to decompose a (normalized) term-document matrix into a set of orthogonal components that are interpreted as latent topics.^[Why TF-IDF and normalization for LSA? LSA works by decomposing the term-document matrix (often a TF-IDF matrix) into a set of orthogonal factors, each of which represents a latent semantic dimension. By normalizing the TF-IDF matrix (e.g., applying L2 normalization to each document vector, as we have here), we make sure that each document contributes equally to the latent space, which is useful when there is variation in document lengths (which we saw in our data processing step). This improves the interpretability of the resulting semantic dimensions.] First, let's create a TF-IDF matrix, which, to oversimplify things, weights word counts by how common they are across documents.


```{python}
tfidf_vectorizer = TfidfVectorizer(strip_accents='ascii', sublinear_tf=True)

tfidf_matrix = tfidf_vectorizer.fit_transform(sampled_speeches['processed_text'])
tfidf_matrix.shape
```

In LSA, it's common practice to normalize the TF-IDF matrix. We'll do that too.

```{python}
normalize = Normalizer()
tfidf_normalized_l2 = normalize.fit_transform(tfidf_matrix)
```

Now let's initialize and fit an SVD model.

```{python}
lsa = TruncatedSVD(n_components=100, n_iter=6, random_state=12)
lsa = lsa.fit(tfidf_normalized_l2) # instead of the unnormalized tfidf_matrix
```

We can inspect the singular values to understand the importance of each latent dimension.

```{python}
svs = lsa.singular_values_[:20]
svs
```

Next, we'll create a dataframe to explore the most important words for each topic.

```{python}
word_topics = pd.DataFrame(lsa.components_).T  # transpose the dataframe so WORDS are in the rows
column_names = [f'Topic {c}' for c in np.arange(1, 101, 1)]
word_topics.columns = column_names

terms = tfidf_vectorizer.get_feature_names_out()
word_topics.index = terms

word_topics.sort_values(by='Topic 2', ascending=False)['Topic 2'].head(20)
```

We can also compare the topic distributions for specific terms.

```{python}
compare_df = pd.DataFrame()
compare_terms = ['england', 'scotland', 'wale', 'ireland']

for i, term in enumerate(compare_terms):
    scores = word_topics.loc[term].sort_values(ascending=False)
    compare_df[i] = scores.index
    compare_df[term] = scores.values

compare_df = compare_df.melt(id_vars=[0], value_vars=compare_terms, var_name='term', value_name='value')
compare_df.rename(columns={0: 'topic'}, inplace=True)

compare_df
```

Let's visualize the topic distributions for each term.

```{python}
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 36))
axes = axes.flatten()

for i, term in enumerate(compare_terms):
    sns.barplot(
        x='value',
        y='topic',
        data=compare_df[compare_df['term'] == term],
        ax=axes[i],
        palette="viridis"
    )
    axes[i].set_title(f'Topics for {term.capitalize()}')
    axes[i].set_xlabel('Value')
    axes[i].set_ylabel('Topic')

plt.tight_layout()
plt.show()
```

- Interpreting these plots involves understanding the association between each term and the latent topics identified by the LSA model.

# Second Wave Topic Models: Latent Dirichlet Allocation (LDA)

Latent Dirichlet Allocation (LDA) is a probabilistic model that treats documents as mixtures of topics and topics as mixtures of words. This allows for a more flexible and nuanced understanding of thematic structures in text data compared to deterministic models like LSA.

Unlike with LSA, we **don't** want to use a TF-IDF matrix, and we don't want to do any normalization. The reason is because LDA is a fully Bayesian generative model, and transforming the count data conflicts with the model's priors; by breaking with assumptions about the data generating process, we end up with less interpretable results.

Let's start by creating a corpus vocabulary with Gensim.

```{python}
sampled_speeches['processed_text_tokenized'] = sampled_speeches['processed_text'].apply(lambda x: x.split())
vocab = corpora.Dictionary(sampled_speeches['processed_text_tokenized'])
vocab.save('output/lda_vocab.dict')

vocab.filter_extremes(no_below=20, no_above=0.95)
corpus = [vocab.doc2bow(text) for text in sampled_speeches['processed_text_tokenized']]
```

Next, we'll fit an LDA model to our data.

```{python}
corpus_sample_size = 1_000

sample_corpus, sample_text = zip(*random.sample(list(zip(corpus,sampled_speeches['processed_text_tokenized'])),corpus_sample_size))

ldamod_s = LdaModel(
    corpus=sample_corpus,
    id2word=vocab,
    num_topics=100,
    random_state=100,
    eval_every=1,
    chunksize=2000,
    alpha='auto',
    eta='auto',
    passes=2,
    update_every=1,
    iterations=400
)
```

We can now explore the topics identified by the LDA model.

```{python}
ldamod_s.get_term_topics('freedom')
```

```{python}
ldamod_s.show_topic(77)
```

```{python}
ldamod_s.get_term_topics('scotland')
```

```{python}
ldamod_s.show_topic(44, topn=10)
```

```{python}
ldamod_s.show_topic(47, topn=10)
```

```{python}
ldamod_s.show_topic(48, topn=10)
```

# Third Wave Topic Models: Transformer-based

Transformers have revolutionized natural language processing in general, and topic modelling is a great example of an area of computational social science that are benefited enormously from these new models.

Unlike the models we've run so far, transformers are almost always at their best with unprocessed data. So, let's return to our unprocessed text data for this example.

```{python}
docs = sampled_speeches['speech']

topic_model = BERTopic()
topics, probs = topic_model.fit_transform(docs)
```

## Topic Assignments and Probabilities

We can explore the topic assignments and their associated probabilities for each document.

```{python}
sampled_speeches['topic'] = topics
sampled_speeches['p(topic)'] = probs
sampled_speeches[['speech', 'topic', 'p(topic)']]
```

```{python}
sampled_speeches
```

## Topic Lookup

We can quickly look up the top words associated with any given topic.

```{python}
topic_model.get_topic(0)
```

## Topic Information

Let's retrieve and explore information about the topics identified by the model.

```{python}
topic_info = topic_model.get_topic_info()
topic_info

print(len(topic_info))

for i in range(len(topic_info)):
    print(f'Topic {i}: {topic_model.get_topic(i)}')
```

```{python}
topic_info
```

## Document Info

Finally, we can get information about how documents are classified into topics.

```{python}
topic_model.get_document_info(docs)
```

We can also visualize the distribution of topics **per document**. BERTopic has a function to do this, `visualize_distribution()`. To see the topic distribution for the first document in our corpus,

```{python}
topic_distr, _ = topic_model.approximate_distribution(sampled_speeches["speech"])
topic_model.visualize_distribution(topic_distr[0])
```

While this is very handy, I'm a bit fastidious when it comes to plotting and... I don't love it. I developed an alternative function, `plot_topic_distribution()` that you might prefer.

```{python}
plot_topic_distribution(topic_distr, topic_model, 0, filename='output/topic_distribution_document_0.png')
```

# Transformer-based Topic Models Applied to the YouTube Data

To end this tutorial, let's apply what we've learned to fit a topic model to the talksatgoogle YouTube data we collected yesterday. We'll be brief and develop this further in the live session.

We'' start by reading in the video description data we collected yesterday.

```{python}
df = pd.read_csv('output/videos.csv')
df.info()
```

We need to do a bit of data prep to create the text text we'll use and removing some irrelevant characters. We'll store the result in a column called `processed_text`.

```{python}
import icsspy.cleaners as clean

metadata_keep = ["snippet.channeltitle" "id", "snippet.title", "contentdetails.licensedcontent", "status.privacystatus", "status.license", "status.publicstatsviewable", "statistics.viewcount", "statistics.likecount", "statistics.favoritecount", "statistics.commentcount"]
remove_substrings = []

texts = clean.merge_title_and_description_strings(
        df, "snippet.title", "snippet.description"
    )

processed_texts, urls = [], []
for text in texts:
    text = text.replace("\n", "")
    text = clean.remove_text_in_brackets(text)

    text, urls_in_text = clean.process_urls(text)
    urls.append(urls_in_text)

    if len(remove_substrings) >= 1:
        text = clean.remove_substrings(text, remove_substrings)
    processed_texts.append(text)

keep_cols = [c for c in df.columns if c in metadata_keep]
df = df[keep_cols]

df["processed_text"] = processed_texts
```

```{python}
yt_docs = df["processed_text"]
```

And now let's fit the topic model!

```{python}
yt_topic_model = BERTopic()
yt_topics, yt_probs = yt_topic_model.fit_transform(yt_docs)
```

```{python}
df['topic'] = yt_topics
df['p(topic)'] = yt_probs

df[['processed_text', 'topic', 'p(topic)']]
```

We can also inspect the topic information dataframe.

```{python}
yt_topic_info = yt_topic_model.get_topic_info()
yt_topic_info
```

Or we can iterate through these topics and print the top words with their weights.

```{python}
for i in range(len(yt_topic_info)):
    print(f'Topic {i}: {yt_topic_model.get_topic(i)}')
```


# Conclusion

As usual, we did a lot here! We learned about simple count-based analysis of manifest content that precede modern latent topic models, and then learned about the most important and influential topic models in several waves of advances in topic modelling. For the first (pre-history) approaches, we learned about difference of proportion analysis. Then we learned about the first wave of topic models by conducting a deterministic Latent Semantic Analysis (LSA). Following LSA, we learned about probabilistic approaches, specifically Latent Dirichlet Allocation (LDA), which was the first Bayesian generative model of latent topics. LDA has dominated topic modelling for decades, but we're on the verge of the third wave: transformer-based topic models. We learned how to fit and interpret transformer-based models using BERTopic.

Finally, we applied what we've learned to a new dataset: YouTube video descriptions. This exercise demonstrated the flexibility of topic models and how they can be applied to various types of text data. As you saw, the differences in the dataset can lead to differences in the topics and their interpretability, highlighting the importance of understanding your data and choosing the right preprocessing and modeling approach.

Throughout this tutorial, I've emphasized the value of understanding how these approaches have evolved, and how they relate to one another. As you continue to explore and apply topic modeling in your work, remember that the choice of method should be guided by your data, your research goals, and an understanding of the assumptions and limitations inherent in each approach. As a next step, you might want to look into the different variations of topic models that have developed from each of these approaches. I especially recommend learning about **structural topic models** and all the extensions of BERTopic, such as semi-supervised and dynamic topic models.

<!--

::: { .aside }
Hi! Is a summary table like this helpful? I'm not sure. Let me know? Thanks!

| What | When | Quantitative Representation | Uses |
| ---- | ---- | --------------------------- | ---- |
| Difference of proportion | Pre-history | Bag-of-words (Counts) | Addition, subtraction |
| LSA | Wave 1 | Normalized TF-IDF | Truncated SVD |
| LDA | Wave 2| Bag-of-words (Counts) | Variational Inference |
| BERTopic | Wave 3 | Contextual Embeddings | UMAP, HDBScan |
:::
 -->
